# Social Intelligence: Emotion prediction in videos

**Project members:** Ved Phadke, Aaron Tae, Madelaine Leitman, Jonah Jung

**Project lead:** Yiling Yun

In this project, we created an interface on Streamlit to predict the emotions of every frame in a video using a series of deep-learning models designed and trained on a dataset called VEATIC. In addition, we evaluated the model from VEATIC by examining the prediction performance in a variety of videos (single-character vs. multi-character, animals & animations vs. humans, landscape vs portrait).

## Medium Article
We have published our findings on Medium, found [here](https://medium.com/@ucladsu/social-intelligence-emotion-prediction-in-videos-01f704a9bfbb).

## Original VEATIC
- The original code is [here](https://veatic.github.io/).

- The VEATIC dataset is [here](https://drive.google.com/file/d/1HZIw8RGsRwwENhJlhNJRL88YyfiE442N/view).

- The pre-trained model is [here](https://drive.google.com/file/d/1dRqmx4UWAtB8E6tcj8XEd16Opk6OZCIx/view).

## Model evaluations
We collected a new set of videos and tested the performance of the model trained using the VEATIC dataset.
- The parameters we obtained from training on all 124 videos for 1-5 epochs are [here](https://drive.google.com/file/d/1koUrsq1aLVsDgiO6otGs5g23Nph8i2dX/view?usp=sharing).

- The videos we collected to test model prediction performance on single-character vs. multi-character, animals & animations vs. humans, and landscape vs portrait videos are [here](https://drive.google.com/file/d/1tFzW_MgCmsV4uqrZNmWhJ491Zs9lEWys/view?usp=sharing).

